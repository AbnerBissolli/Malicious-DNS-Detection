{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_websites = ['facebook',\n",
    "                  'fb',\n",
    "                  'instagram',\n",
    "                  'tiktok',\n",
    "                  'whatsapp',\n",
    "                  'ibm',\n",
    "                  'google',\n",
    "                  'amazon',\n",
    "                  'windows',\n",
    "                  'linux',\n",
    "                  'messsenger',\n",
    "                  'microsoft',\n",
    "                  'twitter',\n",
    "                  'outlook',\n",
    "                  'apple',\n",
    "                  'netflix',\n",
    "                  'flix',\n",
    "                  'film',                  \n",
    "                  'prime',\n",
    "                  'ebay',\n",
    "                  'bunker',\n",
    "                  'play',\n",
    "                  'social',\n",
    "                  'paypal',\n",
    "                  'pypl',\n",
    "                  'win',\n",
    "                 ]\n",
    "\n",
    "words_reliable = ['account',\n",
    "                  'password',\n",
    "                  'passwd',\n",
    "                  'senha',\n",
    "                  'good',\n",
    "                  'secure',\n",
    "                  'security',\n",
    "                  'certified',\n",
    "                  'save',\n",
    "                  'safe',\n",
    "                  'download',\n",
    "                  'down',\n",
    "                  'com',\n",
    "                  'login',\n",
    "                  'register',\n",
    "                  'erro',\n",
    "                  'id',\n",
    "                  'update',\n",
    "                  'submit',\n",
    "                  'oficial',\n",
    "                  'official',\n",
    "                  'home',\n",
    "                  'app',                  \n",
    "                  'web',\n",
    "                  'lock',\n",
    "                  'app',\n",
    "                  'cancel',\n",
    "                  'mobile',\n",
    "                  'copy',\n",
    "                  'warning'\n",
    "                  'warn',\n",
    "                  'verification',\n",
    "                  'verif',\n",
    "                  'recovery',\n",
    "                  'recover',\n",
    "                  'stat',\n",
    "                  'email',\n",
    "                  'reliable',\n",
    "                  'support',\n",
    "                  'doc',\n",
    "                  'notification',\n",
    "                  'notif',                  \n",
    "                  'confirm',\n",
    "                  'key',\n",
    "                  'software',\n",
    "                  'beta',\n",
    "                  'alpha',\n",
    "                  'alfa',\n",
    "                  'user',\n",
    "                  'admin',\n",
    "                  'try',\n",
    "                  'veri',\n",
    "                  'service',\n",
    "                  'import',\n",
    "                  'true',\n",
    "                  'null',\n",
    "                  'my',\n",
    "                  'your',\n",
    "                  'link',\n",
    "                  'online',\n",
    "                  'sign',\n",
    "                  'prof',\n",
    "                  'profile',\n",
    "                  'group',\n",
    "                 ]\n",
    "\n",
    "words_catchy = ['bank',\n",
    "                'money',\n",
    "                'cash',\n",
    "                'game',\n",
    "                'ship',\n",
    "                'market',\n",
    "                'pay',\n",
    "                'new',\n",
    "                'apply',\n",
    "                'deal',\n",
    "                'get',\n",
    "                'now',\n",
    "                'start',\n",
    "                'act',\n",
    "                'free',\n",
    "                'gift',\n",
    "                'card',\n",
    "                'credit',\n",
    "                'hot',\n",
    "                'drug',\n",
    "                'porn',\n",
    "                'ero',\n",
    "                'euro',\n",
    "                'dolar',    \n",
    "                'ofer',\n",
    "                'offer',\n",
    "                'work',\n",
    "                'food',\n",
    "                'book',\n",
    "                'now',\n",
    "                'tech',\n",
    "                'shop',\n",
    "                'diet',\n",
    "                'clinic',\n",
    "                'office',\n",
    "                'blog',\n",
    "                'intern',\n",
    "                'first',\n",
    "                'act',\n",
    "                'refund',\n",
    "                'photo',\n",
    "                'gif',\n",
    "                'net',\n",
    "                'cloud',\n",
    "                'limit',\n",
    "                'vk',\n",
    "               ]\n",
    "\n",
    "words_bad = ['malware',\n",
    "             'spam',\n",
    "             'attack',\n",
    "             'phishing',             \n",
    "            ]\n",
    "\n",
    "words_countries = ['usa', 'unitedstates', 'united', 'states', 'america',\n",
    "                   'brasil', 'brazil', 'bra', 'br',\n",
    "                   'germany', 'ger', 'germ',\n",
    "                   'britain', 'british', 'uk', 'kingdom'                   \n",
    "                   'china', 'chi',\n",
    "                   'india', 'ind',\n",
    "                   'spain', 'sp',\n",
    "                   'italy',\n",
    "                   'france',\n",
    "                   'turkey',\n",
    "                   'poland',\n",
    "                   'russia', 'russ', 'rus',\n",
    "                   'canada', 'can',\n",
    "                   'southkorea', 'sk', 'south','korea',\n",
    "                   'taiwan',\n",
    "                   'japan', 'jp',\n",
    "                   'mexico', 'mex',\n",
    "                   'argentina', 'arg',\n",
    "                   'australia', 'aus',\n",
    "                   'israel', 'isr',\n",
    "                  ]\n",
    "\n",
    "words_rubbish = ['ww',\n",
    "                 'kkk',\n",
    "                 'www',\n",
    "                 'xxx',\n",
    "                 'yyy',\n",
    "                 'zzz',    \n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = words_websites + words_reliable + words_catchy + words_bad + words_countries + words_rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_word(s, w):\n",
    "    if w in s:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def word_val(df_1, df_2, w):\n",
    "    count_1 = sum(df_1['SSD'].apply(have_word, w=w))\n",
    "    count_2 = sum(df_2['SSD'].apply(have_word, w=w))\n",
    "    if count_2 == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    p = count_2/(count_2+count_1)\n",
    "    c = count_2\n",
    "    return p, c\n",
    "\n",
    "def get_words_val(df):\n",
    "    tuples = []\n",
    "    df_be = df[df['class']==1]\n",
    "    df_mw = df[df['class']==0]\n",
    "    for w in target_words:\n",
    "        p, c = word_val(df_be, df_mw, w)\n",
    "        tuples.append((p, c, w))\n",
    "    return tuples\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def have_words(s, words):\n",
    "    bad_words = 0\n",
    "    for word in words:\n",
    "        if word in s:\n",
    "            bad_words +=1\n",
    "    return bad_words\n",
    "\n",
    "def have_words_rplc(s, words):\n",
    "    s = s.replace('0', 'o')\n",
    "    s = s.replace('1', 'i')\n",
    "    s = s.replace('3', 'e')\n",
    "    s = s.replace('4', 'a')\n",
    "    s = s.replace('5', 's')\n",
    "    s = s.replace('6', 'g')\n",
    "    s = s.replace('7', 't')\n",
    "    s = s.replace('8', 'b')\n",
    "    s = s.replace('8', 'b')    \n",
    "    \n",
    "    bad_words = 0\n",
    "    for word in words:\n",
    "        if word in s:\n",
    "            bad_words +=1\n",
    "    return bad_words    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating for Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Dataset/Train/Temp/lx_train.csv')\n",
    "train = train.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 2, 'britain')\n",
      "(0.926829268292683, 38, 'japan')\n",
      "(0.9090909090909091, 30, 'linux')\n",
      "(0.9010989010989011, 82, 'porn')\n",
      "(0.8888888888888888, 104, 'canada')\n",
      "(0.8888888888888888, 8, 'yyy')\n",
      "(0.8823529411764706, 15, 'british')\n",
      "(0.8484848484848485, 28, 'russia')\n",
      "(0.8409090909090909, 37, 'france')\n",
      "(0.8333333333333334, 5, 'taiwan')\n",
      "(0.8333333333333334, 5, 'ibm')\n",
      "(0.8181818181818182, 9, 'israel')\n",
      "(0.8163265306122449, 120, 'euro')\n",
      "(0.8096774193548387, 251, 'game')\n",
      "(0.8076923076923077, 84, 'film')\n",
      "(0.8, 12, 'kkk')\n",
      "(0.8, 4, 'certified')\n",
      "(0.782051282051282, 61, 'software')\n",
      "(0.7738095238095238, 65, 'official')\n",
      "(0.7697841726618705, 107, 'play')\n",
      "(0.8333333333333334, 5, 'attack')\n",
      "(0.7554179566563467, 244, 'blog')\n",
      "(0.75, 27, 'isr')\n",
      "(0.75, 12, 'mexico')\n",
      "(0.7428571428571429, 26, 'alpha')\n",
      "(0.7315914489311164, 616, 'chi')\n",
      "(0.7272727272727273, 16, 'xxx')\n",
      "(0.7258064516129032, 45, 'social')\n",
      "(0.7142857142857143, 5, 'spam')\n",
      "(0.7037037037037037, 38, 'australia')\n",
      "(0.7, 105, 'food')\n",
      "(0.6940024479804161, 567, 'new')\n",
      "(0.6981132075471698, 74, 'good')\n",
      "(0.6923076923076923, 9, 'zzz')\n",
      "(0.6842105263157895, 13, 'korea')\n",
      "(0.68, 17, 'russ')\n",
      "(0.6774193548387096, 42, 'save')\n",
      "(0.6738022426095821, 661, 'sk')\n",
      "(0.670846394984326, 214, 'work')\n",
      "(0.6585365853658537, 27, 'diet')\n",
      "(0.6577060931899642, 367, 'can')\n",
      "(0.6567164179104478, 88, 'try')\n",
      "(0.6551724137931034, 19, 'united')\n",
      "(0.65, 156, 'market')\n",
      "(0.6442141623488774, 373, 'hot')\n",
      "(0.6419437340153452, 1255, 'sp')\n",
      "(0.6756756756756757, 25, 'true')\n",
      "(0.6438356164383562, 47, 'start')\n",
      "(0.6387096774193548, 99, 'aus')\n",
      "(0.6370539104024298, 839, 'br')\n",
      "(0.6379310344827587, 148, 'stat')\n",
      "(0.6231884057971014, 43, 'money')\n",
      "(0.6212121212121212, 41, 'cash')\n",
      "(0.6206896551724138, 216, 'win')\n",
      "(0.6190476190476191, 52, 'ship')\n",
      "(0.6180758017492711, 212, 'home')\n",
      "(0.6166982922201139, 325, 'tech')\n",
      "(0.6133333333333333, 230, 'ero')\n",
      "(0.6111111111111112, 22, 'mex')\n",
      "(0.6108597285067874, 135, 'rus')\n",
      "(0.6119402985074627, 41, 'first')\n",
      "(0.6083916083916084, 87, 'india')\n",
      "(0.603399433427762, 213, 'ger')\n",
      "(0.6009557945041816, 503, 'net')\n",
      "(0.6666666666666666, 4, 'argentina')\n",
      "(0.6065573770491803, 111, 'now')\n",
      "(0.5925925925925926, 32, 'south')\n",
      "(0.5892857142857143, 231, 'shop')\n",
      "(0.5820610687022901, 305, 'uk')\n",
      "(0.6086956521739131, 14, 'prime')\n",
      "(0.5747126436781609, 50, 'key')\n",
      "(0.577259475218659, 198, 'group')\n",
      "(0.5739130434782609, 66, 'prof')\n",
      "(0.5730129390018485, 310, 'ind')\n",
      "(0.5751633986928104, 528, 'online')\n",
      "(0.5714285714285714, 4, 'bunker')\n",
      "(0.5619047619047619, 59, 'down')\n",
      "(0.5556061987237921, 1219, 'id')\n",
      "(0.5573770491803278, 238, 'act')\n",
      "(0.5714285714285714, 8, 'copy')\n",
      "(0.5555555555555556, 50, 'intern')\n",
      "(0.5540540540540541, 82, 'link')\n",
      "(0.538961038961039, 166, 'book')\n",
      "(0.5333333333333333, 8, 'drug')\n",
      "(0.527027027027027, 78, 'jp')\n",
      "(0.5217391304347826, 24, 'clinic')\n",
      "(0.5150214592274678, 120, 'usa')\n",
      "(0.5, 30, 'credit')\n"
     ]
    }
   ],
   "source": [
    "## Defining Bad Words\n",
    "\n",
    "df = train.copy()\n",
    "\n",
    "p_min = 0.5\n",
    "p = 1\n",
    "\n",
    "words = []\n",
    "while p > p_min:\n",
    "    tuples = get_words_val(df)\n",
    "    tuples = sorted(tuples, reverse = True) \n",
    "    \n",
    "    p = tuples[0][0]\n",
    "    w = tuples[0][2]\n",
    "    \n",
    "    df['have_word'] = df['SSD'].apply(have_word, w=w)\n",
    "    df = df[df['have_word']==0]\n",
    "    \n",
    "    if p > p_min:\n",
    "        words.append(w)\n",
    "        \n",
    "    print(tuples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SSD_bad_words'] = train['SSD'].apply(have_words, words=words)\n",
    "train['SUB_bad_words'] = train['SUB'].apply(have_words, words=words)\n",
    "train['SLD_bad_words'] = train['SLD'].apply(have_words, words=words)\n",
    "\n",
    "train['SSD_target_words'] = train['SSD'].apply(have_words, words=target_words)\n",
    "train['SUB_target_words'] = train['SUB'].apply(have_words, words=target_words)\n",
    "train['SLD_target_words'] = train['SLD'].apply(have_words, words=target_words)\n",
    "\n",
    "train['SSD_bad_words_rplc'] = train['SSD'].apply(have_words_rplc, words=words)\n",
    "train['SUB_bad_words_rplc'] = train['SUB'].apply(have_words_rplc, words=words)\n",
    "train['SLD_bad_words_rplc'] = train['SLD'].apply(have_words_rplc, words=words)\n",
    "\n",
    "train['SSD_target_words_rplc'] = train['SSD'].apply(have_words_rplc, words=target_words)\n",
    "train['SUB_target_words_rplc'] = train['SUB'].apply(have_words_rplc, words=target_words)\n",
    "train['SLD_target_words_rplc'] = train['SLD'].apply(have_words_rplc, words=target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "train.to_csv('../Dataset/Train/Temp/lx_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../Dataset/Test/Temp/lx_test.csv')\n",
    "test = test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['SSD_bad_words'] = test['SSD'].apply(have_words, words=words)\n",
    "test['SUB_bad_words'] = test['SUB'].apply(have_words, words=words)\n",
    "test['SLD_bad_words'] = test['SLD'].apply(have_words, words=words)\n",
    "\n",
    "test['SSD_target_words'] = test['SSD'].apply(have_words, words=target_words)\n",
    "test['SUB_target_words'] = test['SUB'].apply(have_words, words=target_words)\n",
    "test['SLD_target_words'] = test['SLD'].apply(have_words, words=target_words)\n",
    "\n",
    "test['SSD_bad_words_rplc'] = test['SSD'].apply(have_words_rplc, words=words)\n",
    "test['SUB_bad_words_rplc'] = test['SUB'].apply(have_words_rplc, words=words)\n",
    "test['SLD_bad_words_rplc'] = test['SLD'].apply(have_words_rplc, words=words)\n",
    "\n",
    "test['SSD_target_words_rplc'] = test['SSD'].apply(have_words_rplc, words=target_words)\n",
    "test['SUB_target_words_rplc'] = test['SUB'].apply(have_words_rplc, words=target_words)\n",
    "test['SLD_target_words_rplc'] = test['SLD'].apply(have_words_rplc, words=target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "test.to_csv('../Dataset/Test/Temp/lx_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
