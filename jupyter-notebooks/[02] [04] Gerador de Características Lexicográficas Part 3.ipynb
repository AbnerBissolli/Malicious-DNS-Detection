{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_websites = ['facebook',\n",
    "                  'fb',\n",
    "                  'instagram',\n",
    "                  'tiktok',\n",
    "                  'whatsapp',\n",
    "                  'ibm',\n",
    "                  'google',\n",
    "                  'amazon',\n",
    "                  'windows',\n",
    "                  'linux',\n",
    "                  'messsenger',\n",
    "                  'microsoft',\n",
    "                  'twitter',\n",
    "                  'outlook',\n",
    "                  'apple',\n",
    "                  'netflix',\n",
    "                  'flix',\n",
    "                  'film',                  \n",
    "                  'prime',\n",
    "                  'ebay',\n",
    "                  'bunker',\n",
    "                  'play',\n",
    "                  'social',\n",
    "                  'paypal',\n",
    "                  'pypl',\n",
    "                  'win',\n",
    "                 ]\n",
    "\n",
    "words_reliable = ['account',\n",
    "                  'password',\n",
    "                  'passwd',\n",
    "                  'senha',\n",
    "                  'good',\n",
    "                  'secure',\n",
    "                  'security',\n",
    "                  'certified',\n",
    "                  'save',\n",
    "                  'safe',\n",
    "                  'download',\n",
    "                  'down',\n",
    "                  'com',\n",
    "                  'login',\n",
    "                  'register',\n",
    "                  'erro',\n",
    "                  'id',\n",
    "                  'update',\n",
    "                  'submit',\n",
    "                  'oficial',\n",
    "                  'official',\n",
    "                  'home',\n",
    "                  'app',                  \n",
    "                  'web',\n",
    "                  'lock',\n",
    "                  'app',\n",
    "                  'cancel',\n",
    "                  'mobile',\n",
    "                  'copy',\n",
    "                  'warning'\n",
    "                  'warn',\n",
    "                  'verification',\n",
    "                  'verif',\n",
    "                  'recovery',\n",
    "                  'recover',\n",
    "                  'stat',\n",
    "                  'email',\n",
    "                  'reliable',\n",
    "                  'support',\n",
    "                  'doc',\n",
    "                  'notification',\n",
    "                  'notif',                  \n",
    "                  'confirm',\n",
    "                  'key',\n",
    "                  'software',\n",
    "                  'beta',\n",
    "                  'alpha',\n",
    "                  'alfa',\n",
    "                  'user',\n",
    "                  'admin',\n",
    "                  'try',\n",
    "                  'veri',\n",
    "                  'service',\n",
    "                  'import',\n",
    "                  'true',\n",
    "                  'null',\n",
    "                  'my',\n",
    "                  'your',\n",
    "                  'link',\n",
    "                  'online',\n",
    "                  'sign',\n",
    "                  'prof',\n",
    "                  'profile',\n",
    "                  'group',\n",
    "                 ]\n",
    "\n",
    "words_catchy = ['bank',\n",
    "                'money',\n",
    "                'cash',\n",
    "                'game',\n",
    "                'ship',\n",
    "                'market',\n",
    "                'pay',\n",
    "                'new',\n",
    "                'apply',\n",
    "                'deal',\n",
    "                'get',\n",
    "                'now',\n",
    "                'start',\n",
    "                'act',\n",
    "                'free',\n",
    "                'gift',\n",
    "                'card',\n",
    "                'credit',\n",
    "                'hot',\n",
    "                'drug',\n",
    "                'porn',\n",
    "                'ero',\n",
    "                'euro',\n",
    "                'dolar',    \n",
    "                'ofer',\n",
    "                'offer',\n",
    "                'work',\n",
    "                'food',\n",
    "                'book',\n",
    "                'now',\n",
    "                'tech',\n",
    "                'shop',\n",
    "                'diet',\n",
    "                'clinic',\n",
    "                'office',\n",
    "                'blog',\n",
    "                'intern',\n",
    "                'first',\n",
    "                'act',\n",
    "                'refund',\n",
    "                'photo',\n",
    "                'gif',\n",
    "                'net',\n",
    "                'cloud',\n",
    "                'limit',\n",
    "                'vk',\n",
    "               ]\n",
    "\n",
    "words_bad = ['malware',\n",
    "             'spam',\n",
    "             'attack',\n",
    "             'phishing',             \n",
    "            ]\n",
    "\n",
    "words_countries = ['usa', 'unitedstates', 'united', 'states', 'america',\n",
    "                   'brasil', 'brazil', 'bra', 'br',\n",
    "                   'germany', 'ger', 'germ',\n",
    "                   'britain', 'british', 'uk', 'kingdom'                   \n",
    "                   'china', 'chi',\n",
    "                   'india', 'ind',\n",
    "                   'spain', 'sp',\n",
    "                   'italy',\n",
    "                   'france',\n",
    "                   'turkey',\n",
    "                   'poland',\n",
    "                   'russia', 'russ', 'rus',\n",
    "                   'canada', 'can',\n",
    "                   'southkorea', 'sk', 'south','korea',\n",
    "                   'taiwan',\n",
    "                   'japan', 'jp',\n",
    "                   'mexico', 'mex',\n",
    "                   'argentina', 'arg',\n",
    "                   'australia', 'aus',\n",
    "                   'israel', 'isr',\n",
    "                  ]\n",
    "\n",
    "words_rubbish = ['ww',\n",
    "                 'kkk',\n",
    "                 'www',\n",
    "                 'xxx',\n",
    "                 'yyy',\n",
    "                 'zzz',    \n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = words_websites + words_reliable + words_catchy + words_bad + words_countries + words_rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_word(s, w):\n",
    "    if w in s:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def word_val(df_1, df_2, w):\n",
    "    count_1 = sum(df_1['SSD'].apply(have_word, w=w))\n",
    "    count_2 = sum(df_2['SSD'].apply(have_word, w=w))\n",
    "    if count_2 == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    p = count_2/(count_2+count_1)\n",
    "    c = count_2\n",
    "    return p, c\n",
    "\n",
    "def get_words_val(df):\n",
    "    tuples = []\n",
    "    df_be = df[df['class']==1]\n",
    "    df_mw = df[df['class']==0]\n",
    "    for w in target_words:\n",
    "        p, c = word_val(df_be, df_mw, w)\n",
    "        tuples.append((p, c, w))\n",
    "    return tuples\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def have_words(s, words):\n",
    "    bad_words = 0\n",
    "    for word in words:\n",
    "        if word in s:\n",
    "            bad_words +=1\n",
    "    return bad_words\n",
    "\n",
    "def have_words_rplc(s, words):\n",
    "    s = s.replace('0', 'o')\n",
    "    s = s.replace('1', 'i')\n",
    "    s = s.replace('3', 'e')\n",
    "    s = s.replace('4', 'a')\n",
    "    s = s.replace('5', 's')\n",
    "    s = s.replace('6', 'g')\n",
    "    s = s.replace('7', 't')\n",
    "    s = s.replace('8', 'b')\n",
    "    s = s.replace('8', 'b')    \n",
    "    \n",
    "    bad_words = 0\n",
    "    for word in words:\n",
    "        if word in s:\n",
    "            bad_words +=1\n",
    "    return bad_words    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating for Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Dataset/Train/Temp/lx_train.csv')\n",
    "train = train.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 3, 'britain')\n",
      "(1.0, 2, 'yyy')\n",
      "(1.0, 2, 'poland')\n",
      "(0.9333333333333333, 14, 'linux')\n",
      "(0.875, 7, 'ibm')\n",
      "(0.847457627118644, 50, 'porn')\n",
      "(0.8181818181818182, 9, 'british')\n",
      "(0.8095238095238095, 17, 'russia')\n",
      "(0.8, 20, 'japan')\n",
      "(0.75, 3, 'taiwan')\n",
      "(0.75, 3, 'spam')\n",
      "(0.75, 3, 'attack')\n",
      "(0.746268656716418, 50, 'canada')\n",
      "(0.7046632124352331, 136, 'game')\n",
      "(0.6707317073170732, 55, 'euro')\n",
      "(0.6666666666666666, 34, 'official')\n",
      "(0.6666666666666666, 12, 'xxx')\n",
      "(0.6666666666666666, 6, 'kkk')\n",
      "(0.6612903225806451, 41, 'film')\n",
      "(0.625, 5, 'argentina')\n",
      "(0.6153846153846154, 16, 'france')\n",
      "(0.6067415730337079, 54, 'play')\n",
      "(0.6, 6, 'mexico')\n",
      "(0.5909090909090909, 13, 'united')\n",
      "(0.5902439024390244, 121, 'blog')\n",
      "(0.574468085106383, 27, 'social')\n",
      "(0.5714285714285714, 4, 'israel')\n",
      "(0.5625, 27, 'software')\n",
      "(0.5625, 9, 'russ')\n",
      "(0.5584415584415584, 43, 'good')\n",
      "(0.5564053537284895, 291, 'chi')\n",
      "(0.5555555555555556, 20, 'australia')\n",
      "(0.5526315789473685, 126, 'work')\n",
      "(0.5357142857142857, 15, 'diet')\n",
      "(0.5314814814814814, 287, 'new')\n",
      "(0.53, 53, 'food')\n",
      "(0.5217391304347826, 12, 'alpha')\n",
      "(0.5023474178403756, 321, 'sk')\n",
      "(0.5172413793103449, 15, 'true')\n",
      "(0.5, 9, 'isr')\n"
     ]
    }
   ],
   "source": [
    "## Defining Bad Words\n",
    "\n",
    "df = train.copy()\n",
    "\n",
    "p_min = 0.5\n",
    "p = 1\n",
    "\n",
    "words = []\n",
    "while p > p_min:\n",
    "    tuples = get_words_val(df)\n",
    "    tuples = sorted(tuples, reverse = True) \n",
    "    \n",
    "    p = tuples[0][0]\n",
    "    w = tuples[0][2]\n",
    "    \n",
    "    df['have_word'] = df['SSD'].apply(have_word, w=w)\n",
    "    df = df[df['have_word']==0]\n",
    "    \n",
    "    if p > p_min:\n",
    "        words.append(w)\n",
    "        \n",
    "    print(tuples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SSD_bad_words'] = train['SSD'].apply(have_words, words=words)\n",
    "train['SUB_bad_words'] = train['SUB'].apply(have_words, words=words)\n",
    "train['SLD_bad_words'] = train['SLD'].apply(have_words, words=words)\n",
    "\n",
    "train['SSD_target_words'] = train['SSD'].apply(have_words, words=target_words)\n",
    "train['SUB_target_words'] = train['SUB'].apply(have_words, words=target_words)\n",
    "train['SLD_target_words'] = train['SLD'].apply(have_words, words=target_words)\n",
    "\n",
    "train['SSD_bad_words_rplc'] = train['SSD'].apply(have_words_rplc, words=words)\n",
    "train['SUB_bad_words_rplc'] = train['SUB'].apply(have_words_rplc, words=words)\n",
    "train['SLD_bad_words_rplc'] = train['SLD'].apply(have_words_rplc, words=words)\n",
    "\n",
    "train['SSD_target_words_rplc'] = train['SSD'].apply(have_words_rplc, words=target_words)\n",
    "train['SUB_target_words_rplc'] = train['SUB'].apply(have_words_rplc, words=target_words)\n",
    "train['SLD_target_words_rplc'] = train['SLD'].apply(have_words_rplc, words=target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "train.to_csv('../Dataset/Train/Temp/lx_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../Dataset/Test/Temp/lx_test.csv')\n",
    "test = test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['SSD_bad_words'] = test['SSD'].apply(have_words, words=words)\n",
    "test['SUB_bad_words'] = test['SUB'].apply(have_words, words=words)\n",
    "test['SLD_bad_words'] = test['SLD'].apply(have_words, words=words)\n",
    "\n",
    "test['SSD_target_words'] = test['SSD'].apply(have_words, words=target_words)\n",
    "test['SUB_target_words'] = test['SUB'].apply(have_words, words=target_words)\n",
    "test['SLD_target_words'] = test['SLD'].apply(have_words, words=target_words)\n",
    "\n",
    "test['SSD_bad_words_rplc'] = test['SSD'].apply(have_words_rplc, words=words)\n",
    "test['SUB_bad_words_rplc'] = test['SUB'].apply(have_words_rplc, words=words)\n",
    "test['SLD_bad_words_rplc'] = test['SLD'].apply(have_words_rplc, words=words)\n",
    "\n",
    "test['SSD_target_words_rplc'] = test['SSD'].apply(have_words_rplc, words=target_words)\n",
    "test['SUB_target_words_rplc'] = test['SUB'].apply(have_words_rplc, words=target_words)\n",
    "test['SLD_target_words_rplc'] = test['SLD'].apply(have_words_rplc, words=target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "test.to_csv('../Dataset/Test/Temp/lx_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
