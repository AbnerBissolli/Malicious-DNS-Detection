{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating for Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../Dataset/Train/raw_train.pickle')\n",
    "\n",
    "train = train.drop(columns=['Third Party'])\n",
    "train = train.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "\n",
    "def get_len(s):\n",
    "    return len(s)\n",
    "\n",
    "################################################################\n",
    "\n",
    "def get_pct(s):\n",
    "    numbers = sum(c.isdigit() for c in s)\n",
    "    lenght  = len(s)\n",
    "    if numbers == 0:\n",
    "        return 0\n",
    "    return numbers/lenght\n",
    "\n",
    "################################################################\n",
    "\n",
    "def get_nan(s):\n",
    "    numbers = sum(c.isdigit() for c in s)\n",
    "    letters = sum(c.isalpha() for c in s)    \n",
    "    others  = len(s) - numbers - letters\n",
    "    return others\n",
    "\n",
    "################################################################\n",
    "\n",
    "def get_etp(s):\n",
    "    if s == '':\n",
    "        return 0\n",
    "    \n",
    "    pd_series = pd.Series(list(s))\n",
    "    counts = pd_series.value_counts()\n",
    "    return entropy(counts)\n",
    "\n",
    "################################################################\n",
    "\n",
    "def get_nos(s):\n",
    "    word_list =s.split('.')\n",
    "    return len(word_list)\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SSD_len'] = train['SSD'].apply(get_len)\n",
    "train['SUB_len'] = train['SUB'].apply(get_len)\n",
    "train['SLD_len'] = train['SLD'].apply(get_len)\n",
    "\n",
    "train['SSD_pct'] = train['SSD'].apply(get_pct)\n",
    "train['SUB_pct'] = train['SUB'].apply(get_pct)\n",
    "train['SLD_pct'] = train['SLD'].apply(get_pct)\n",
    "\n",
    "train['SSD_nan'] = train['SSD'].apply(get_nan)\n",
    "train['SUB_nan'] = train['SUB'].apply(get_nan)\n",
    "train['SLD_nan'] = train['SLD'].apply(get_nan)\n",
    "\n",
    "train['SSD_etp'] = train['SSD'].apply(get_etp)\n",
    "train['SUB_etp'] = train['SUB'].apply(get_etp)\n",
    "train['SLD_etp'] = train['SLD'].apply(get_etp)\n",
    "\n",
    "train['SUB_num'] = train['SUB'].apply(get_nos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "\n",
    "def get_val_1(s):\n",
    "    total = len(s) \n",
    "    if total == 0:\n",
    "        return 0     \n",
    "    \n",
    "    s = sorted(Counter(s), key=Counter(s).get, reverse=True)\n",
    "    \n",
    "    count = 0 \n",
    "    for c in s:\n",
    "        count += s.count(c)\n",
    "        if count/total > 0.5:\n",
    "            return count\n",
    "    return count\n",
    "\n",
    "################################################################\n",
    "\n",
    "def get_val_2(s):\n",
    "    total = len(s) \n",
    "    if total == 0:\n",
    "        return 0    \n",
    "    \n",
    "    s = sorted(Counter(s), key=Counter(s).get, reverse=True)\n",
    "\n",
    "    count = 0   \n",
    "    for c in s[:5]:\n",
    "        count += s.count(c)\n",
    "    return count/total\n",
    "\n",
    "################################################################\n",
    "\n",
    "def get_val_3(s):\n",
    "    total = len(s) \n",
    "    if total == 0:\n",
    "        return ''     \n",
    "    \n",
    "    s = sorted(Counter(s), key=Counter(s).get, reverse=True)\n",
    "    return s[0]\n",
    "\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SSD_val_1'] = train['SSD'].apply(get_val_1)\n",
    "train['SUB_val_1'] = train['SUB'].apply(get_val_1)\n",
    "train['SLD_val_1'] = train['SLD'].apply(get_val_1)\n",
    "\n",
    "train['SSD_val_2'] = train['SSD'].apply(get_val_2)\n",
    "train['SUB_val_2'] = train['SUB'].apply(get_val_2)\n",
    "train['SLD_val_2'] = train['SLD'].apply(get_val_2)\n",
    "\n",
    "train['SSD_val_3'] = train['SSD'].apply(get_val_3)\n",
    "train['SUB_val_3'] = train['SUB'].apply(get_val_3)\n",
    "train['SLD_val_3'] = train['SLD'].apply(get_val_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "train.to_csv('../Dataset/Train/Temp/lx_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle('../Dataset/Test/raw_test.pickle')\n",
    "\n",
    "test = test.drop(columns=['Third Party'])\n",
    "test = test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['SSD_len'] = test['SSD'].apply(get_len)\n",
    "test['SUB_len'] = test['SUB'].apply(get_len)\n",
    "test['SLD_len'] = test['SLD'].apply(get_len)\n",
    "\n",
    "test['SSD_pct'] = test['SSD'].apply(get_pct)\n",
    "test['SUB_pct'] = test['SUB'].apply(get_pct)\n",
    "test['SLD_pct'] = test['SLD'].apply(get_pct)\n",
    "\n",
    "test['SSD_nan'] = test['SSD'].apply(get_nan)\n",
    "test['SUB_nan'] = test['SUB'].apply(get_nan)\n",
    "test['SLD_nan'] = test['SLD'].apply(get_nan)\n",
    "\n",
    "test['SSD_etp'] = test['SSD'].apply(get_etp)\n",
    "test['SUB_etp'] = test['SUB'].apply(get_etp)\n",
    "test['SLD_etp'] = test['SLD'].apply(get_etp)\n",
    "\n",
    "test['SUB_num'] = test['SUB'].apply(get_nos)\n",
    "\n",
    "test['SSD_val_1'] = test['SSD'].apply(get_val_1)\n",
    "test['SUB_val_1'] = test['SUB'].apply(get_val_1)\n",
    "test['SLD_val_1'] = test['SLD'].apply(get_val_1)\n",
    "\n",
    "test['SSD_val_2'] = test['SSD'].apply(get_val_2)\n",
    "test['SUB_val_2'] = test['SUB'].apply(get_val_2)\n",
    "test['SLD_val_2'] = test['SLD'].apply(get_val_2)\n",
    "\n",
    "test['SSD_val_3'] = test['SSD'].apply(get_val_3)\n",
    "test['SUB_val_3'] = test['SUB'].apply(get_val_3)\n",
    "test['SLD_val_3'] = test['SLD'].apply(get_val_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "test.to_csv('../Dataset/Test/Temp/lx_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_websites = ['facebook',\n",
    "                  'fb',\n",
    "                  'instagram',\n",
    "                  'tiktok',\n",
    "                  'whatsapp',\n",
    "                  'ibm',\n",
    "                  'google',\n",
    "                  'amazon',\n",
    "                  'windows',\n",
    "                  'linux',\n",
    "                  'messsenger',\n",
    "                  'microsoft',\n",
    "                  'twitter',\n",
    "                  'outlook',\n",
    "                  'apple',\n",
    "                  'netflix',\n",
    "                  'flix',\n",
    "                  'film',                  \n",
    "                  'prime',\n",
    "                  'ebay',\n",
    "                  'bunker',\n",
    "                  'play',\n",
    "                  'social',\n",
    "                  'paypal',\n",
    "                  'pypl',\n",
    "                  'win',\n",
    "                 ]\n",
    "\n",
    "words_reliable = ['account',\n",
    "                  'password',\n",
    "                  'passwd',\n",
    "                  'senha',\n",
    "                  'good',\n",
    "                  'secure',\n",
    "                  'security',\n",
    "                  'certified',\n",
    "                  'save',\n",
    "                  'safe',\n",
    "                  'download',\n",
    "                  'down',\n",
    "                  'com',\n",
    "                  'login',\n",
    "                  'register',\n",
    "                  'erro',\n",
    "                  'id',\n",
    "                  'update',\n",
    "                  'submit',\n",
    "                  'oficial',\n",
    "                  'official',\n",
    "                  'home',\n",
    "                  'app',                  \n",
    "                  'web',\n",
    "                  'lock',\n",
    "                  'app',\n",
    "                  'cancel',\n",
    "                  'mobile',\n",
    "                  'copy',\n",
    "                  'warning'\n",
    "                  'warn',\n",
    "                  'verification',\n",
    "                  'verif',\n",
    "                  'recovery',\n",
    "                  'recover',\n",
    "                  'stat',\n",
    "                  'email',\n",
    "                  'reliable',\n",
    "                  'support',\n",
    "                  'doc',\n",
    "                  'notification',\n",
    "                  'notif',                  \n",
    "                  'confirm',\n",
    "                  'key',\n",
    "                  'software',\n",
    "                  'beta',\n",
    "                  'alpha',\n",
    "                  'alfa',\n",
    "                  'user',\n",
    "                  'admin',\n",
    "                  'try',\n",
    "                  'veri',\n",
    "                  'service',\n",
    "                  'import',\n",
    "                  'true',\n",
    "                  'null',\n",
    "                  'my',\n",
    "                  'your',\n",
    "                  'link',\n",
    "                  'online',\n",
    "                  'sign',\n",
    "                  'prof',\n",
    "                  'profile',\n",
    "                  'group',\n",
    "                 ]\n",
    "\n",
    "words_catchy = ['bank',\n",
    "                'money',\n",
    "                'cash',\n",
    "                'game',\n",
    "                'ship',\n",
    "                'market',\n",
    "                'pay',\n",
    "                'new',\n",
    "                'apply',\n",
    "                'deal',\n",
    "                'get',\n",
    "                'now',\n",
    "                'start',\n",
    "                'act',\n",
    "                'free',\n",
    "                'gift',\n",
    "                'card',\n",
    "                'credit',\n",
    "                'hot',\n",
    "                'drug',\n",
    "                'porn',\n",
    "                'ero',\n",
    "                'euro',\n",
    "                'dolar',    \n",
    "                'ofer',\n",
    "                'offer',\n",
    "                'work',\n",
    "                'food',\n",
    "                'book',\n",
    "                'now',\n",
    "                'tech',\n",
    "                'shop',\n",
    "                'diet',\n",
    "                'clinic',\n",
    "                'office',\n",
    "                'blog',\n",
    "                'intern',\n",
    "                'first',\n",
    "                'act',\n",
    "                'refund',\n",
    "                'photo',\n",
    "                'gif',\n",
    "                'net',\n",
    "                'cloud',\n",
    "                'limit',\n",
    "                'vk',\n",
    "               ]\n",
    "\n",
    "words_bad = ['malware',\n",
    "             'spam',\n",
    "             'attack',\n",
    "             'phishing',             \n",
    "            ]\n",
    "\n",
    "words_countries = ['usa', 'unitedstates', 'united', 'states', 'america',\n",
    "                   'brasil', 'brazil', 'bra', 'br',\n",
    "                   'germany', 'ger', 'germ',\n",
    "                   'britain', 'british', 'uk', 'kingdom'                   \n",
    "                   'china', 'chi',\n",
    "                   'india', 'ind',\n",
    "                   'spain', 'sp',\n",
    "                   'italy',\n",
    "                   'france',\n",
    "                   'turkey',\n",
    "                   'poland',\n",
    "                   'russia', 'russ', 'rus',\n",
    "                   'canada', 'can',\n",
    "                   'southkorea', 'sk', 'south','korea',\n",
    "                   'taiwan',\n",
    "                   'japan', 'jp',\n",
    "                   'mexico', 'mex',\n",
    "                   'argentina', 'arg',\n",
    "                   'australia', 'aus',\n",
    "                   'israel', 'isr',\n",
    "                  ]\n",
    "\n",
    "words_rubbish = ['ww',\n",
    "                 'kkk',\n",
    "                 'www',\n",
    "                 'xxx',\n",
    "                 'yyy',\n",
    "                 'zzz',    \n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = words_websites + words_reliable + words_catchy + words_bad + words_rubbish + words_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_word(s, w):\n",
    "    if w in s:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def word_val(df_1, df_2, w):\n",
    "    count_1 = sum(df_1['SSD'].apply(have_word, w=w))\n",
    "    count_2 = sum(df_2['SSD'].apply(have_word, w=w))\n",
    "    \n",
    "    if count_2 == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    p = count_2/(count_2+count_1)\n",
    "    c = count_2\n",
    "    return p, c\n",
    "\n",
    "def get_words_val(df):\n",
    "    tuples = []    \n",
    "    df_be = df[df['class']==1]\n",
    "    df_mw = df[df['class']==0]\n",
    "    for w in target_words:\n",
    "        p, c = word_val(df_be, df_mw, w)\n",
    "        tuples.append((p, c, w))\n",
    "    return tuples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 130, 'confirm')\n",
      "(1.0, 42, 'notification')\n",
      "(1.0, 26, 'refund')\n",
      "(1.0, 13, 'cancel')\n",
      "(1.0, 4, 'senha')\n",
      "(1.0, 4, 'pypl')\n",
      "(1.0, 4, 'phishing')\n",
      "(1.0, 2, 'null')\n",
      "(1.0, 2, 'malware')\n",
      "(1.0, 1, 'unitedstates')\n",
      "(1.0, 1, 'oficial')\n",
      "(1.0, 1, 'messsenger')\n",
      "(0.9908256880733946, 324, 'paypal')\n",
      "(0.9810606060606061, 259, 'login')\n",
      "(0.9790940766550522, 281, 'verif')\n",
      "(0.975, 351, 'account')\n",
      "(0.9742840444184687, 1667, 'www')\n",
      "(0.972972972972973, 36, 'flix')\n",
      "(0.96, 288, 'secure')\n",
      "(1.0, 2, 'reliable')\n",
      "(0.96, 24, 'outlook')\n",
      "(0.9534883720930233, 123, 'microsoft')\n",
      "(0.9523809523809523, 20, 'instagram')\n",
      "(0.9369369369369369, 104, 'facebook')\n",
      "(0.9357142857142857, 131, 'update')\n",
      "(0.9272727272727272, 51, 'amazon')\n",
      "(0.9183673469387755, 270, 'apple')\n",
      "(0.8953488372093024, 77, 'recovery')\n",
      "(0.8846153846153846, 23, 'recover')\n",
      "(0.8571428571428571, 6, 'notif')\n",
      "(0.8333333333333334, 30, 'ebay')\n",
      "(0.8333333333333334, 5, 'apply')\n",
      "(0.8163265306122449, 40, 'ofer')\n",
      "(0.8159362549800797, 1024, 'app')\n",
      "(0.8028169014084507, 57, 'limit')\n",
      "(0.7941176470588235, 27, 'email')\n",
      "(0.782312925170068, 115, 'support')\n",
      "(0.7543859649122807, 43, 'admin')\n",
      "(0.7349397590361446, 122, 'mobile')\n",
      "(0.7316017316017316, 338, 'bank')\n",
      "(0.7, 7, 'profile')\n",
      "(0.6881720430107527, 64, 'security')\n",
      "(0.6875, 22, 'windows')\n",
      "(0.6875, 22, 'brasil')\n",
      "(0.6666666666666666, 12, 'register')\n",
      "(0.6666666666666666, 4, 'submit')\n",
      "(0.6625310173697271, 267, 'service')\n",
      "(0.6557377049180327, 320, 'get')\n",
      "(0.6352941176470588, 108, 'vk')\n",
      "(0.6268656716417911, 42, 'user')\n",
      "(0.6258992805755396, 87, 'cloud')\n",
      "(0.625, 5, 'brazil')\n",
      "(0.6111111111111112, 11, 'beta')\n",
      "(0.6086956521739131, 28, 'alfa')\n",
      "(0.6037735849056604, 64, 'safe')\n",
      "(0.6, 12, 'copy')\n",
      "(0.5897435897435898, 115, 'pay')\n",
      "(0.5611111111111111, 101, 'fb')\n",
      "(0.5555555555555556, 70, 'card')\n",
      "(0.5555555555555556, 25, 'erro')\n",
      "(0.5365853658536586, 22, 'google')\n",
      "(0.5263157894736842, 10, 'offer')\n",
      "(0.5249500998003992, 263, 'sign')\n",
      "(0.5223880597014925, 35, 'office')\n",
      "(0.5153061224489796, 505, 'com')\n",
      "(0.5, 21, 'gift')\n"
     ]
    }
   ],
   "source": [
    "## Defining Bad Words\n",
    "\n",
    "df = train.copy()\n",
    "\n",
    "p_min = 0.5\n",
    "p = 1\n",
    "\n",
    "words = []\n",
    "while p > p_min:\n",
    "    tuples = get_words_val(df)\n",
    "    tuples = sorted(tuples, reverse = True) \n",
    "    \n",
    "    p = tuples[0][0]\n",
    "    w = tuples[0][2]\n",
    "    \n",
    "    df['have_word'] = df['SSD'].apply(have_word, w=w)\n",
    "    df = df[df['have_word']==0]\n",
    "    \n",
    "    if p > p_min:\n",
    "        words.append(w)\n",
    "        \n",
    "    print(tuples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_words(s, words):\n",
    "    bad_words = 0\n",
    "    for word in words:\n",
    "        if word in s:\n",
    "            bad_words +=1\n",
    "    return bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SSD_bad_words'] = train['SSD'].apply(have_words, words=words)\n",
    "train['SUB_bad_words'] = train['SUB'].apply(have_words, words=words)\n",
    "train['SLD_bad_words'] = train['SLD'].apply(have_words, words=words)\n",
    "\n",
    "train['SSD_target_words'] = train['SSD'].apply(have_words, words=target_words)\n",
    "train['SUB_target_words'] = train['SUB'].apply(have_words, words=target_words)\n",
    "train['SLD_target_words'] = train['SLD'].apply(have_words, words=target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_words_rplc(s, words):\n",
    "    s = s.replace('0', 'o')\n",
    "    s = s.replace('1', 'i')\n",
    "    s = s.replace('3', 'e')\n",
    "    s = s.replace('4', 'a')\n",
    "    s = s.replace('5', 's')\n",
    "    s = s.replace('6', 'g')\n",
    "    s = s.replace('7', 't')\n",
    "    s = s.replace('8', 'b')\n",
    "    s = s.replace('8', 'b')    \n",
    "    \n",
    "    bad_words = 0\n",
    "    for word in words:\n",
    "        if word in s:\n",
    "            bad_words +=1\n",
    "    return bad_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SSD_bad_words_rplc'] = train['SSD'].apply(have_words_rplc, words=words)\n",
    "train['SUB_bad_words_rplc'] = train['SUB'].apply(have_words_rplc, words=words)\n",
    "train['SLD_bad_words_rplc'] = train['SLD'].apply(have_words_rplc, words=words)\n",
    "\n",
    "train['SSD_target_words_rplc'] = train['SSD'].apply(have_words_rplc, words=target_words)\n",
    "train['SUB_target_words_rplc'] = train['SUB'].apply(have_words_rplc, words=target_words)\n",
    "train['SLD_target_words_rplc'] = train['SLD'].apply(have_words_rplc, words=target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "train.to_csv('../Dataset/Train/Temp/lx_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['SSD_bad_words'] = test['SSD'].apply(have_words, words=words)\n",
    "test['SUB_bad_words'] = test['SUB'].apply(have_words, words=words)\n",
    "test['SLD_bad_words'] = test['SLD'].apply(have_words, words=words)\n",
    "\n",
    "test['SSD_target_words'] = test['SSD'].apply(have_words, words=target_words)\n",
    "test['SUB_target_words'] = test['SUB'].apply(have_words, words=target_words)\n",
    "test['SLD_target_words'] = test['SLD'].apply(have_words, words=target_words)\n",
    "\n",
    "test['SSD_bad_words_rplc'] = test['SSD'].apply(have_words_rplc, words=words)\n",
    "test['SUB_bad_words_rplc'] = test['SUB'].apply(have_words_rplc, words=words)\n",
    "test['SLD_bad_words_rplc'] = test['SLD'].apply(have_words_rplc, words=words)\n",
    "\n",
    "test['SSD_target_words_rplc'] = test['SSD'].apply(have_words_rplc, words=target_words)\n",
    "test['SUB_target_words_rplc'] = test['SUB'].apply(have_words_rplc, words=target_words)\n",
    "test['SLD_target_words_rplc'] = test['SLD'].apply(have_words_rplc, words=target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "test.to_csv('../Dataset/Test/Temp/lx_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def dig_seq_l(s):\n",
    "    if len(s) == 0:\n",
    "        return 0\n",
    "    \n",
    "    res = [''.join(j).strip() for k, j in groupby(s, str.isdigit)]\n",
    "    res = [sub for sub in res if sub.isdigit()]\n",
    "    \n",
    "    if len(res) == 0:\n",
    "        return 0\n",
    "    \n",
    "    res = max(res, key = len)\n",
    "    return len(res)\n",
    "\n",
    "def chr_seq_l(s):\n",
    "    if len(s) == 0:\n",
    "        return 0\n",
    "    \n",
    "    res = [''.join(g) for _, g in groupby(s)]\n",
    "    res = max(res, key = len)\n",
    "    return len(res)\n",
    "\n",
    "def chr_seq_c(s):\n",
    "    if len(s) == 0:\n",
    "        return ''    \n",
    "    \n",
    "    res = [''.join(g) for _, g in groupby(s)]\n",
    "    res = max(res, key = len)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SSD_dig_seq_l'] = train['SSD'].apply(dig_seq_l)\n",
    "train['SUB_dig_seq_l'] = train['SUB'].apply(dig_seq_l)\n",
    "train['SLD_dig_seq_l'] = train['SLD'].apply(dig_seq_l)\n",
    "\n",
    "train['SSD_chr_seq_l'] = train['SSD'].apply(chr_seq_l)\n",
    "train['SUB_chr_seq_l'] = train['SUB'].apply(chr_seq_l)\n",
    "train['SLD_chr_seq_l'] = train['SLD'].apply(chr_seq_l)\n",
    "\n",
    "train['SSD_chr_seq_c'] = train['SSD'].apply(chr_seq_c)\n",
    "train['SUB_chr_seq_c'] = train['SUB'].apply(chr_seq_c)\n",
    "train['SLD_chr_seq_c'] = train['SLD'].apply(chr_seq_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "train.to_csv('../Dataset/Train/Temp/lx_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['SSD_dig_seq_l'] = test['SSD'].apply(dig_seq_l)\n",
    "test['SUB_dig_seq_l'] = test['SUB'].apply(dig_seq_l)\n",
    "test['SLD_dig_seq_l'] = test['SLD'].apply(dig_seq_l)\n",
    "\n",
    "test['SSD_chr_seq_l'] = test['SSD'].apply(chr_seq_l)\n",
    "test['SUB_chr_seq_l'] = test['SUB'].apply(chr_seq_l)\n",
    "test['SLD_chr_seq_l'] = test['SLD'].apply(chr_seq_l)\n",
    "\n",
    "test['SSD_chr_seq_c'] = test['SSD'].apply(chr_seq_c)\n",
    "test['SUB_chr_seq_c'] = test['SUB'].apply(chr_seq_c)\n",
    "test['SLD_chr_seq_c'] = test['SLD'].apply(chr_seq_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset\n",
    "\n",
    "test.to_csv('../Dataset/Test/Temp/lx_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
